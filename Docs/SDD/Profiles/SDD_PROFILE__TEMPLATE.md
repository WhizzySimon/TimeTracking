# SDD Profile Template (for any target)
**Last updated:** 2025-12-24  
**Purpose:** Create a comparable “SDD/SDD profile” for a system, vendor guidance, or repo.

## Target
- **Name:**
- **Type:** repo / vendor docs / workflow system / transcript / other
- **Scope:** what is included/excluded in this profile
- **Version / commit / date window:**
- **Analyst:** (human/agent)

## Evidence inventory
> List only things you actually looked at (files, docs pages, transcripts).
- Source 1:
- Source 2:
- …

## Scoring policy
- **Axes:** 10 axes below
- **Score meaning depends on target type:**
  - **Repo/workflow system:** “implemented maturity” (0 = absent, 5 = robust + enforced)
  - **Vendor docs:** “guidance coverage” (0 = not covered, 5 = detailed + actionable)
- **Confidence:** High / Med / Low based on evidence directness

## Axis score table
| Axis | Score (0–5) | Confidence | Evidence pointers | 1‑line rationale |
|---|---:|---|---|---|
| 1. Spec quality |  |  |  |  |
| 2. Scope control |  |  |  |  |
| 3. Traceability |  |  |  |  |
| 4. Code quality rules |  |  |  |  |
| 5. Testing strategy |  |  |  |  |
| 6. Tooling & automation |  |  |  |  |
| 7. Context bootstrapping |  |  |  |  |
| 8. Iteration loop |  |  |  |  |
| 9. Operational safety |  |  |  |  |
| 10. Cost/time efficiency |  |  |  |  |

## Key practices (max 10)
1.
2.
…

## Failure modes / typical defects (max 10)
1.
2.
…

## “Best-of” adoptable practices for TimeTracker (max 10)
> Concrete changes you would port into TimeTracker (name files/paths if applicable).
1.
2.
…

## Open questions / gaps
- Gap 1:
- Gap 2:

## Notes
- Anything that might bias scores (missing docs, private scripts, etc.)
